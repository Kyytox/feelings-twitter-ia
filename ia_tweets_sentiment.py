# -*- coding: utf-8 -*-
"""IA-Tweets-Sentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mC8E4rEpdmiVlAXDJ30nto8vmr9yXD3D
"""

# TEXTBLOB 
# !pip install git+https://github.com/tweepy/tweepy.git --upgrade

import tweepy
import pandas as pd
import json
import time
import re
from textblob import TextBlob
from textblob_fr import PatternTagger, PatternAnalyzer

def nlp_pipeline(text):
    # remove alla caract usefull for help textBlob
    text = text.lower()
    text = text.replace('\n', ' ').replace('\r', '')
    text = ' '.join(text.split())
    text = re.sub(r"[A-Za-z\.]*[0-9]+[A-Za-z%°\.]*", "", text)
    text = re.sub(r"(\s\-\s|-$)", "", text)
    text = re.sub(r"[?\%\(\)\/\|\"]", "", text)
    text = re.sub(r"\&\S*\s", "", text)
    text = re.sub(r"\&", "", text)
    text = re.sub(r"\+", "", text)
    text = re.sub(r"\#", "", text)
    text = re.sub(r"\$", "", text)
    text = re.sub(r"\£", "", text)
    text = re.sub(r"\%", "", text)
    text = re.sub(r"\:", "", text)
    text = re.sub(r"\@", "", text)
    text = re.sub(r"\-", "", text)

    return text

# Initialization
auth = tweepy.OAuthHandler("","")
auth.set_access_token("-","")
api = tweepy.API(auth)

polarity = ""


#kytox id = 1106506057235525632
#cerfiaFR id = 971820228

# Calling DataFrame constructor
df = pd.DataFrame()

# user = api.get_user(screen_name='CerfiaFR')
# print(user)

# Retriev tweets by Users, with extended mode and don't take replie tweets 
# tweets = api.user_timeline(user_id=971820228, tweet_mode='extended', exclude_replies=True, count=20)

# for x in tweets:
#   tw_text = nlp_pipeline(x.full_text)
#   print(tw_text)

#   # determin if the tweets is negative or positive
#   # -1 = negative -- +1 = positive
#   polarity = TextBlob(tw_text,pos_tagger=PatternTagger(),analyzer=PatternAnalyzer()).sentiment[0]
#   print(polarity)


t = "il est mort"
polarity = TextBlob(t,pos_tagger=PatternTagger(),analyzer=PatternAnalyzer()).sentiment
print(polarity)


t = "viol d’une fille de 6 ans"
polarity = TextBlob(t,pos_tagger=PatternTagger(),analyzer=PatternAnalyzer()).sentiment
print(polarity)


t = "within a few days rape of a 6-year-old girl by a 16-year-old teenager."
polarity = TextBlob(t,pos_tagger=PatternTagger(),analyzer=PatternAnalyzer()).sentiment
print(polarity)


# df = pd.DataFrame(tweets)
# print(df)

# NLTK 
# !pip install git+https://github.com/tweepy/tweepy.git --upgrade
# !pip install vaderSentiment-fr

import tweepy
import pandas as pd
import json
import time
import re
import nltk
from nltk import tokenize
from nltk.sentiment import SentimentAnalyzer
from nltk.sentiment.util import *
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from vaderSentiment_fr.vaderSentiment import SentimentIntensityAnalyzer


# nltk.download('vader_lexicon')
# nltk.download("omw-1.4")
# nltk.download("sentence_polarity")
# nltk.download('punkt')
# nltk.download([
#     "names",
#     "stopwords",
#     "state_union",
#     "twitter_samples",
#     "movie_reviews",
#     "averaged_perceptron_tagger",
#     "vader_lexicon",
#     "punkt",
# ])

SIA = SentimentIntensityAnalyzer()

def nlp_pipeline(text):
    # remove alla caract usefull for help textBlob
    text = text.lower()
    text = text.replace('\n', ' ').replace('\r', '')
    text = ' '.join(text.split())
    # text = re.sub(r"[A-Za-z\.]*[0-9]+[A-Za-z%°\.]*", "", text)
    text = re.sub(r"(\s\-\s|-$)", "", text)
    text = re.sub(r"[?\%\(\)\/\|\"]", "", text)
    text = re.sub(r"\&\S*\s", "", text)
    text = re.sub(r"\&", "", text)
    text = re.sub(r"\+", "", text)
    text = re.sub(r"\#", "", text)
    text = re.sub(r"\$", "", text)
    text = re.sub(r"\£", "", text)
    text = re.sub(r"\%", "", text)
    text = re.sub(r"\:", "", text)
    text = re.sub(r"\@", "", text)
    text = re.sub(r"\-", "", text)

    return text

# Initialization
auth = tweepy.OAuthHandler("","")
auth.set_access_token("-","")
api = tweepy.API(auth)

polarity = ""

#kytox id = 1106506057235525632
#cerfiaFR id = 971820228

# Calling DataFrame constructor
df = pd.DataFrame()

# user = api.get_user(screen_name='CerfiaFR')
# print(user)

# Retriev tweets by Users, with extended mode and don't take replie tweets 
# tweets = api.user_timeline(user_id=971820228, tweet_mode='extended', exclude_replies=True, count=20)

# for x in tweets:
#   tw_text = nlp_pipeline(x.full_text)
#   print(tw_text)
  
#   sid = SentimentIntensityAnalyzer()
#   ss = sid.polarity_scores(tw_text)

#   for k in sorted(ss):
#     print('{0}: {1}, '.format(k, ss[k]), end='')
#     print()



t = "En l’espace de quelques jours : Viol d’une fille de 6 ans par un ado de 16 ans. Lola, 12 ans, tuée et retrouvée dans une malle après viols et tortures. Justine, 20 ans, mère d’un enfant de 2 ans, violée, tuée, à la sortie d’une discothèque."
# sid = SentimentIntensityAnalyzer()
# ss = sid.polarity_scores(t)
# for k in sorted(ss):
#   print('{0}: {1}, '.format(k, ss[k]), end='')
#   print()

print(SIA.polarity_scores(t))



t = "In the space of a few days: Rape of a 6-year-old girl by a 16-year-old teenager. Lola, 12, killed and found in a trunk after rape and torture. Justine, 20, mother of a 2-year-old child, raped, killed, leaving a nightclub."
sid = SentimentIntensityAnalyzer()
ss = sid.polarity_scores(t)
for k in sorted(ss):
  print('{0}: {1}, '.format(k, ss[k]), end='')
  print()



# df = pd.DataFrame(tweets)
# print(df)

# BERT - Twitter-roBERTa-base
# !pip install transformers>=4.0
# !pip install sentencepiece

import tensorflow as tf
assert tf.__version__ >= "2.0"

from transformers import AutoModelForSequenceClassification
from transformers import TFAutoModelForSequenceClassification
from transformers import AutoTokenizer, AutoConfig
import numpy as np
from scipy.special import softmax

task='sentiment'
MODEL = "cardiffnlp/twitter-xlm-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
config = AutoConfig.from_pretrained(MODEL)

model = AutoModelForSequenceClassification.from_pretrained(MODEL)
model.save_pretrained(MODEL)
# nlp = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)

import tweepy
import pandas as pd
import json
import time


# Initialization
auth = tweepy.OAuthHandler("","")
auth.set_access_token("-","")
api = tweepy.API(auth)

#kytox id = 1106506057235525632
#cerfiaFR id = 971820228

# Calling DataFrame constructor
df = pd.DataFrame(columns=['Sentiment', 'Score', 'Id_Tweet', 'Negative', 'Neutral', 'Positive', 'Text_Tweet', 'Date_Tweet'])


# user = api.get_user(screen_name='CerfiaFR')
# print(user)

sauv_id = 1537879027175673858

# id dernier tweet
# sauv_id = 1585726779607502849

for i in range(0,1):
  print(i)
  # Retriev tweets by Users, with extended mode and don't take replie tweets 
  tweets = api.user_timeline(user_id=971820228, max_id=sauv_id, tweet_mode='extended', exclude_replies=True, count=200)
  # tweets = api.user_timeline(user_id=971820228, tweet_mode='extended', exclude_replies=True, count=1000)

  if tweets:
    # test = list(tweets)
    # print(test)
    # # len = len(test)
    # print(len)
    sauv_id = tweets[-1].id

    # Remove first element because he is in the CSV
    # Active when the first collect is OK
    tweets.pop(0)

    for x in tweets:
      # detect Text sentiment 
      encoded_input = tokenizer(x.full_text, return_tensors='pt')
      output = model(**encoded_input)
      scores = output[0][0].detach().numpy()

      # Format outup : [Negative - Neutral - Positive]
      scores = softmax(scores)

      # Collect the max score and the sentment
      ranking = np.argsort(scores)
      ranking = ranking[::-1]

      l = config.id2label[ranking[0]]
      s = scores[ranking[0]]
      st_score = np.round(float(s), 3)

      # insert in DataFrame
      df = df.append({'Sentiment': l,'Score': st_score, 'Id_Tweet': x.id, 'Negative': scores[0], 'Neutral': scores[1], 'Positive': scores[2],
                      'Text_Tweet': x.full_text, 'Date_Tweet': x.created_at}, ignore_index=True)



print(df)

    # # # append data frame to CSV file
    # if i == 0 :
    #   df.to_csv('out.csv', index=False, header=True)
    # else :
# df.to_csv('out.csv', mode='a', index=False, header=False)